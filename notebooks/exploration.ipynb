{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ae5a7c",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # Project Genesis - Phase 1: Exploration Notebook\n",
    "# \n",
    "# Este notebook é nosso playground para testar ideias iniciais de geração de código com IA.\n",
    "# %%\n",
    "# Instalações iniciais (normalmente já fizemos via requirements.txt)\n",
    "# !pip install transformers torch datasets numpy\n",
    "\n",
    "# %%\n",
    "# Importações essenciais\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Verifica se temos uma GPU (acelera MUITO o treinamento)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Dispositivo de treinamento: {device}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Carregando um Dataset de Exemplo\n",
    "# \n",
    "# Vamos usar o `CodeSearchNet` (ou um subconjunto) para treinar nosso modelo. Ele contém pares de (função, docstring).\n",
    "\n",
    "# %%\n",
    "# Carrega um dataset pequeno para experimentação\n",
    "# Em um projeto real, usaríamos o dataset completo ou um customizado\n",
    "try:\n",
    "    dataset = load_dataset(\"code_search_net\", \"python\", split='train[:1%]') # Pega 1% dos dados para teste\n",
    "    print(f\"Dataset carregado. Amostras: {len(dataset)}\")\n",
    "    print(f\"Exemplo de entrada: {dataset[0]['func_code_string'][:200]}...\")  # Código\n",
    "    print(f\"Exemplo de docstring: {dataset[0]['func_documentation_string'][:100]}...\") # Descrição\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o dataset: {e}\")\n",
    "    # Fallback para um dataset mais simples se houver erro\n",
    "    dataset = load_dataset(\"lvwerra/codeparrot-clean\", split='train[:1000]')\n",
    "    print(f\"Usando dataset fallback. Amostras: {len(dataset)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Pré-processamento dos Dados\n",
    "# \n",
    "# Precisamos tokenizar o texto (transformar palavras em números que o modelo entende).\n",
    "\n",
    "# %%\n",
    "# Escolha um modelo de linguagem pré-treinado como ponto de partida.\n",
    "# 'microsoft/DialoGPT-small' é pequeno e bom para testes. Em produção, usaríamos um modelo de código.\n",
    "model_checkpoint = \"microsoft/DialoGPT-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Define o token de padding\n",
    "\n",
    "# Função para tokenizar os exemplos do dataset\n",
    "def tokenize_function(examples):\n",
    "    # Combine a docstring (objetivo) com o código (resposta) para criar o texto de entrada.\n",
    "    # Formato: \"Gere um código que: <DOCSTRING> \\n Código: <CODE>\"\n",
    "    texts = [f\"Gere um código que: {doc} \\n Código: {code}\" for doc, code in zip(examples['func_documentation_string'], examples['func_code_string'])]\n",
    "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Aplica a tokenização a todo o dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Divide o dataset em treino e teste\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Carregando e Configurando o Modelo de IA\n",
    "\n",
    "# %%\n",
    "# Carrega um modelo de linguagem causal (para geração de texto)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Configurando o Treinamento\n",
    "\n",
    "# %%\n",
    "# Define os argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs/models/test_trainer\",  # Onde salvar os checkpoints\n",
    "    evaluation_strategy=\"epoch\",                 # Avalia a cada epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,              # Tamanho do lote (reduza se faltar memória)\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,                         # Apenas 1 epoch para teste RÁPIDO\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./outputs/logs/\",              # Para logs do TensorBoard\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Cria o objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Treinamento! (Isso vai demorar)\n",
    "# \n",
    "# **AVISO:** Este é um treinamento de TESTE. O modelo resultante será muito ruim, mas provará o conceito.\n",
    "\n",
    "# %%\n",
    "# Inicia o treinamento\n",
    "print(\"Iniciando treinamento...\")\n",
    "trainer.train()\n",
    "\n",
    "# Salva o modelo e o tokenizer após o treinamento\n",
    "trainer.save_model(\"./outputs/models/first_test_model\")\n",
    "tokenizer.save_pretrained(\"./outputs/models/first_test_model\")\n",
    "print(\"Modelo salvo em './outputs/models/first_test_model'\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Teste de Geração de Código\n",
    "# \n",
    "# Vamos ver o que nosso modelo \"aprendeu\".\n",
    "\n",
    "# %%\n",
    "# Função para gerar código a partir de um prompt\n",
    "def generate_code(prompt, model, tokenizer, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Prompt de teste\n",
    "test_prompt = \"Gere um código que: Calculate the sum of a list of numbers. \\n Código:\"\n",
    "generated_code = generate_code(test_prompt, model, tokenizer)\n",
    "print(\"Código gerado:\\n\", generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5825c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Project Genesis - Phase 1: Exploration Notebook\n",
    "# \n",
    "# Este notebook é nosso playground para testar ideias iniciais de geração de código com IA.\n",
    "# %%\n",
    "# Instalações iniciais (normalmente já fizemos via requirements.txt)\n",
    "# !pip install transformers torch datasets numpy\n",
    "\n",
    "# %%\n",
    "# Importações essenciais\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Verifica se temos uma GPU (acelera MUITO o treinamento)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Dispositivo de treinamento: {device}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Carregando um Dataset de Exemplo\n",
    "# \n",
    "# Vamos usar o `CodeSearchNet` (ou um subconjunto) para treinar nosso modelo. Ele contém pares de (função, docstring).\n",
    "\n",
    "# %%\n",
    "# Carrega um dataset pequeno para experimentação\n",
    "# Em um projeto real, usaríamos o dataset completo ou um customizado\n",
    "try:\n",
    "    dataset = load_dataset(\"code_search_net\", \"python\", split='train[:1%]') # Pega 1% dos dados para teste\n",
    "    print(f\"Dataset carregado. Amostras: {len(dataset)}\")\n",
    "    print(f\"Exemplo de entrada: {dataset[0]['func_code_string'][:200]}...\")  # Código\n",
    "    print(f\"Exemplo de docstring: {dataset[0]['func_documentation_string'][:100]}...\") # Descrição\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o dataset: {e}\")\n",
    "    # Fallback para um dataset mais simples se houver erro\n",
    "    dataset = load_dataset(\"lvwerra/codeparrot-clean\", split='train[:1000]')\n",
    "    print(f\"Usando dataset fallback. Amostras: {len(dataset)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Pré-processamento dos Dados\n",
    "# \n",
    "# Precisamos tokenizar o texto (transformar palavras em números que o modelo entende).\n",
    "\n",
    "# %%\n",
    "# Escolha um modelo de linguagem pré-treinado como ponto de partida.\n",
    "# 'microsoft/DialoGPT-small' é pequeno e bom para testes. Em produção, usaríamos um modelo de código.\n",
    "model_checkpoint = \"microsoft/DialoGPT-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Define o token de padding\n",
    "\n",
    "# Função para tokenizar os exemplos do dataset\n",
    "def tokenize_function(examples):\n",
    "    # Combine a docstring (objetivo) com o código (resposta) para criar o texto de entrada.\n",
    "    # Formato: \"Gere um código que: <DOCSTRING> \\n Código: <CODE>\"\n",
    "    texts = [f\"Gere um código que: {doc} \\n Código: {code}\" for doc, code in zip(examples['func_documentation_string'], examples['func_code_string'])]\n",
    "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Aplica a tokenização a todo o dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Divide o dataset em treino e teste\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Carregando e Configurando o Modelo de IA\n",
    "\n",
    "# %%\n",
    "# Carrega um modelo de linguagem causal (para geração de texto)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Configurando o Treinamento\n",
    "\n",
    "# %%\n",
    "# Define os argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs/models/test_trainer\",  # Onde salvar os checkpoints\n",
    "    evaluation_strategy=\"epoch\",                 # Avalia a cada epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,              # Tamanho do lote (reduza se faltar memória)\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,                         # Apenas 1 epoch para teste RÁPIDO\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./outputs/logs/\",              # Para logs do TensorBoard\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Cria o objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Treinamento! (Isso vai demorar)\n",
    "# \n",
    "# **AVISO:** Este é um treinamento de TESTE. O modelo resultante será muito ruim, mas provará o conceito.\n",
    "\n",
    "# %%\n",
    "# Inicia o treinamento\n",
    "print(\"Iniciando treinamento...\")\n",
    "trainer.train()\n",
    "\n",
    "# Salva o modelo e o tokenizer após o treinamento\n",
    "trainer.save_model(\"./outputs/models/first_test_model\")\n",
    "tokenizer.save_pretrained(\"./outputs/models/first_test_model\")\n",
    "print(\"Modelo salvo em './outputs/models/first_test_model'\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Teste de Geração de Código\n",
    "# \n",
    "# Vamos ver o que nosso modelo \"aprendeu\".\n",
    "\n",
    "# %%\n",
    "# Função para gerar código a partir de um prompt\n",
    "def generate_code(prompt, model, tokenizer, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Prompt de teste\n",
    "test_prompt = \"Gere um código que: Calculate the sum of a list of numbers. \\n Código:\"\n",
    "generated_code = generate_code(test_prompt, model, tokenizer)\n",
    "print(\"Código gerado:\\n\", generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33db56",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
